{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d6bd46-1b31-44bc-b35f-06d1fbaf9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data\n",
    "# predict category from title/abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab12b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwinonuonga/env/llm-arm64/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import typing as t\n",
    "from ast import literal_eval\n",
    "\n",
    "from transformer.models.classifier import ClassifierLM\n",
    "from transformer.dataloaders.inference import InferenceDataModule\n",
    "from transformer.params import TransformerParams\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e78e563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>['cs.CV', 'cs.IR']</td>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "      <td>Despite the growing availability of big data i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']</td>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "      <td>This paper presents a simple end-to-end model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "      <td>The popular Q-learning algorithm is known to o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>['stat.ML', 'cs.LG', 'math.OC']</td>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "      <td>Principal components analysis (PCA) is a well-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'stat.ML']</td>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "      <td>SDYNA is a general framework designed to addre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             terms  \\\n",
       "56176                           ['cs.CV', 'cs.IR']   \n",
       "56177  ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']   \n",
       "56178                                    ['cs.LG']   \n",
       "56179              ['stat.ML', 'cs.LG', 'math.OC']   \n",
       "56180                ['cs.LG', 'cs.AI', 'stat.ML']   \n",
       "\n",
       "                                                  titles  \\\n",
       "56176  Mining Spatio-temporal Data on Industrializati...   \n",
       "56177  Wav2Letter: an End-to-End ConvNet-based Speech...   \n",
       "56178  Deep Reinforcement Learning with Double Q-lear...   \n",
       "56179                        Generalized Low Rank Models   \n",
       "56180  Chi-square Tests Driven Method for Learning th...   \n",
       "\n",
       "                                               abstracts  \n",
       "56176  Despite the growing availability of big data i...  \n",
       "56177  This paper presents a simple end-to-end model ...  \n",
       "56178  The popular Q-learning algorithm is known to o...  \n",
       "56179  Principal components analysis (PCA) is a well-...  \n",
       "56180  SDYNA is a general framework designed to addre...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and preview data\n",
    "data = pd.read_csv(\"data/arxiv.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535a4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get titles and primary category\n",
    "X = data.titles.to_list()\n",
    "y = data.terms.apply(literal_eval).str[0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69825956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categories\n",
    "label_encoder = LabelEncoder()\n",
    "y = torch.from_numpy(label_encoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6594f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data module\n",
    "class ArxivDataModule(InferenceDataModule):\n",
    "    def setup(self: t.Self, stage: str) -> None:\n",
    "        self.X, self.y = X, y\n",
    "        super().setup(stage=stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92febe62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize pretrained tokenizer\n",
    "# - llama does not add an EOS token by default, so override this\n",
    "# - llama also does not use a padding token, so this needs to be added\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\", add_eos_token=True, legacy=False\n",
    ")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "374b7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the transformer\n",
    "context_length = 64\n",
    "model = ClassifierLM(\n",
    "    config=TransformerParams(context_length=context_length),\n",
    "    tokenizer=tokenizer,\n",
    "    num_classes=len(y.unique())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b20b6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize & encode data and prepare train/test splits\n",
    "datamodule = ArxivDataModule(\n",
    "    tokenizer=tokenizer,\n",
    "    context_length=context_length,\n",
    "    batch_size=32,\n",
    "    val_size=0.2,\n",
    "    test_size=0.1,\n",
    "    num_workers=9,\n",
    "    persistent_workers=True,\n",
    "    limit=1000,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc4cb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | ModuleDict | 35.3 M | train\n",
      "---------------------------------------------\n",
      "35.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.3 M    Total params\n",
      "141.165   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwinonuonga/env/llm-arm64/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  73%|███████▎  | 16/22 [00:02<00:00,  7.91it/s, v_num=8, val_loss=0.867, train_loss=0.868]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edwinonuonga/env/llm-arm64/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer = Trainer(\n",
    "    max_epochs=500,\n",
    "    callbacks=EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "    accelerator=\"gpu\",\n",
    ")\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4cad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test metrics\n",
    "trainer.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b525af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view first batch of test set predictions\n",
    "pred = trainer.predict(model=model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
