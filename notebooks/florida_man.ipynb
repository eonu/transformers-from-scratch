{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c4c691-9f4b-4975-9e03-c8a42c90df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/bcruise/adventures-of-florida-man/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67bc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "\n",
    "from transformer.models.causal import CausalLM\n",
    "from transformer.dataloaders.teacher_forcing import TeacherForcingDataModule\n",
    "from transformer.params import TransformerParams\n",
    "\n",
    "import pandas as pd\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from transformers import LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d506536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42768    Florida woman assaults boyfriend after he refu...\n",
       "42769    Florida Woman Arrested After Dispute Over Moth...\n",
       "42770    Law firm demands Florida man remove racist ‘co...\n",
       "42771    Florida Man arrested for assaulting wife with ...\n",
       "42772    Half of the articles linked in /r/FloridaMan d...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and preview data\n",
    "titles = pd.read_csv(\"data/florida_man.csv\").title\n",
    "titles.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df59e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data module\n",
    "class FloridaManDataModule(TeacherForcingDataModule):\n",
    "    def setup(self: t.Self, stage: str) -> None:\n",
    "        # read titles with 200 or fewer characters from CSV\n",
    "        self.data = titles.loc[titles.str.len() <= 200].to_list()\n",
    "        super().setup(stage=stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e12da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize pretrained tokenizer for causal language modelling\n",
    "# - llama does not add an EOS token by default, so override this\n",
    "# - llama also does not use a padding token, so this needs to be added\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\", add_eos_token=True, legacy=False\n",
    ")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d3f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the transformer\n",
    "context_length = 64\n",
    "model = CausalLM(\n",
    "    config=TransformerParams(context_length=context_length),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89332135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize & encode data and prepare train/test splits\n",
    "datamodule = FloridaManDataModule(\n",
    "    tokenizer=tokenizer,\n",
    "    context_length=context_length,\n",
    "    batch_size=32,\n",
    "    val_size=0.2,\n",
    "    test_size=0.1,\n",
    "    num_workers=9,\n",
    "    persistent_workers=True,\n",
    "    limit=None,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc96abad",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "No supported gpu backend found!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model\u001b[38;5;241m=\u001b[39mmodel, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n",
      "File \u001b[0;32m~/env/transformer/lib/python3.11/site-packages/lightning/pytorch/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/transformer/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:400\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# init connectors\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector \u001b[38;5;241m=\u001b[39m _DataConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_connector \u001b[38;5;241m=\u001b[39m \u001b[43m_AcceleratorConnector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync_batchnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync_batchnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_distributed_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplugins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector \u001b[38;5;241m=\u001b[39m _LoggerConnector(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_connector \u001b[38;5;241m=\u001b[39m _CallbackConnector(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/env/transformer/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:143\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_choose_auto_accelerator()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerator_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_choose_gpu_accelerator_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_device_config_and_set_final_flags(devices\u001b[38;5;241m=\u001b[39mdevices, num_nodes\u001b[38;5;241m=\u001b[39mnum_nodes)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_parallel_devices_and_init_accelerator()\n",
      "File \u001b[0;32m~/env/transformer/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:353\u001b[0m, in \u001b[0;36m_AcceleratorConnector._choose_gpu_accelerator_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CUDAAccelerator\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo supported gpu backend found!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: No supported gpu backend found!"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer = Trainer(\n",
    "    max_epochs=500,\n",
    "    callbacks=EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "    accelerator=\"cpu\",\n",
    ")\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test metrics\n",
    "trainer.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45a4dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 4/4 [00:01<00:00,  2.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# view first batch of test set predictions\n",
    "# note: these are still produced using teacher-forcing, so not purely generated\n",
    "pred = trainer.predict(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a6b0d124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bin різ go trip testimsaksakդTimestampTimestamp manSERVERSERVERSERVER'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05550d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Florida man Denkmallanglefern'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.generate(\"Florida man\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
